{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2738fcf",
   "metadata": {},
   "source": [
    "# 11 — Optimización Académica del AUC-ROC\n",
    "\n",
    "Este notebook amplía el proyecto incorporando un módulo experimental cuyo único objetivo es **empujar el AUC-ROC** más allá del benchmark actual (0.8593) utilizando los mismos datos, features y pipelines previamente validados. Nos enfocamos en investigación de modelos, no en repetir el EDA ni el preprocesamiento ya documentado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4a6f1",
   "metadata": {},
   "source": [
    "## Objetivo y Plan\n",
    "\n",
    "- Reutilizar el **pipeline oficial**: `SimpleImputer → SMOTE → StandardScaler → Modelo`.\n",
    "- Evaluar **modelos originales** diseñados específicamente para este proyecto:\n",
    "  - **Cost-Sensitive XGBoost**: función de pérdida custom basada en costos de negocio\n",
    "  - **Monotonic LightGBM**: restricciones de monotonicidad según conocimiento de dominio\n",
    "  - **Hybrid Voting System**: ensamble EBM + XGBoost con ponderación optimizable\n",
    "  - **Balanced Random Patches**: ensamble custom con doble aleatoriedad\n",
    "\n",
    "  - **Stacking Compacto**: meta-learner con base learners diversos- Generar tabla comparativa, curva ROC del ganador y guardar el modelo como `best_auc_model.pkl` en `models/`.\n",
    "\n",
    "- Afinar hiperparámetros con **Optuna (20-30 trials)** por modelo usando `StratifiedKFold(k=5)`.- Registrar `AUC-ROC` de validación, `AUC-ROC` en test y el *gap* entre ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a1be756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección 0: Configuración y Librerías\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay, classification_report, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.utils import resample\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "ROOT = Path(r\"c:\\\\MachineLearningPG\")\n",
    "DATA_PATH = ROOT / \"data\" / \"processed_for_modeling.csv\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TARGET = \"SeriousDlqin2yrs\"\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_TRIALS = 25\n",
    "STUDY_TIMEOUT = 900 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220b00f",
   "metadata": {},
   "source": [
    "### Sección 0 — Configuración Inicial\n",
    "En esta celda reunimos todas las dependencias clave, establecemos rutas del proyecto y fijamos parámetros globales (semillas, número de folds, límites de Optuna). De este modo mantenemos centralizada la configuración antes de iniciar cualquier entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55281c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset listo → Train: (105000, 16), Test: (45000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Sección 1: Carga de Datos y Split Base\n",
    "assert DATA_PATH.exists(), f\"No se encontró {DATA_PATH}\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset listo → Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "def _normalize_feature_name(name: str) -> str:\n",
    "    return ''.join(ch for ch in name.lower() if ch.isalnum())\n",
    "\n",
    "_MONOTONIC_RULES = {\n",
    "    \"revolvingutilizationofunsecuredlines\": 1,\n",
    "    \"age\": -1,\n",
    "    \"numberoftime3059dayspastduenotworse\": 1,\n",
    "    \"debtratio\": 1,\n",
    "    \"monthlyincome\": -1,\n",
    "    \"numberofopencreditlinesandloans\": 0,\n",
    "    \"numberoftimes90dayslate\": 1,\n",
    "    \"numberrealestateloansorlines\": 0,\n",
    "    \"numberoftime6089dayspastduenotworse\": 1,\n",
    "    \"numberofdependents\": 0\n",
    "}\n",
    "\n",
    "MONOTONIC_CONSTRAINTS = [\n",
    "    _MONOTONIC_RULES.get(_normalize_feature_name(col), 0)\n",
    "    for col in X_train.columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f91d42",
   "metadata": {},
   "source": [
    "### Sección 1 — División de Datos\n",
    "Leemos el dataset preprocesado, limpiamos columnas residuales y generamos el split estratificado 70/30 que se reutiliza en todos los experimentos para garantizar comparabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fada68b",
   "metadata": {},
   "source": [
    "### Balanced Random Patches (BRP)\n",
    "Este ensamble poco común entrena múltiples clasificadores sobre **subconjuntos aleatorios de observaciones y features** simultáneamente, reduciendo la correlación entre modelos y mejorando la estabilidad del AUC. No existe implementación directa en `scikit-learn`, por lo que construimos una versión ligera basada en árboles balanceados para aportar una opción verdaderamente novedosa al portafolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865a6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedRandomPatches(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Ensamble custom inspirado en Random Patches con balanceo estratificado.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator=None,\n",
    "        n_estimators=25,\n",
    "        max_samples=0.6,\n",
    "        max_features=0.6,\n",
    "        random_state=None\n",
    "    ):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.estimators_ = []\n",
    "        self.feature_indices_ = []\n",
    "\n",
    "        X_arr = np.asarray(X)\n",
    "        n_samples = X_arr.shape[0]\n",
    "        n_features = X_arr.shape[1]\n",
    "        sample_size = max(1, int(self.max_samples * n_samples))\n",
    "        feature_size = max(1, int(self.max_features * n_features))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            feat_idx = rng.choice(n_features, size=feature_size, replace=False)\n",
    "            X_subset = X_arr[:, feat_idx]\n",
    "            X_bal, y_bal = resample(\n",
    "                X_subset,\n",
    "                y,\n",
    "                n_samples=sample_size,\n",
    "                stratify=y,\n",
    "                random_state=rng.randint(0, 10_000)\n",
    "            )\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_bal, y_bal)\n",
    "            self.estimators_.append(estimator)\n",
    "            self.feature_indices_.append(feat_idx)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_arr = np.asarray(X)\n",
    "        proba = np.zeros((X_arr.shape[0], len(self.classes_)))\n",
    "        for est, feat_idx in zip(self.estimators_, self.feature_indices_):\n",
    "            proba += est.predict_proba(X_arr[:, feat_idx])\n",
    "        proba /= len(self.estimators_)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22251a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades compartidas\n",
    "cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def make_pipeline(estimator):\n",
    "    return ImbPipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", estimator),\n",
    "    ])\n",
    "\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_data, y_data):\n",
    "    proba = pipeline.predict_proba(X_data)[:, 1]\n",
    "    return roc_auc_score(y_data, proba)\n",
    "\n",
    "\n",
    "def objective_factory(builder):\n",
    "    def objective(trial):\n",
    "        estimator = builder(trial)\n",
    "        pipeline = make_pipeline(estimator)\n",
    "        train_scores, val_scores = [], []\n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            pipeline.fit(X_tr, y_tr)\n",
    "            val_scores.append(evaluate_pipeline(pipeline, X_val, y_val))\n",
    "            train_scores.append(evaluate_pipeline(pipeline, X_tr, y_tr))\n",
    "        trial.set_user_attr(\"train_auc\", float(np.mean(train_scores)))\n",
    "        return float(np.mean(val_scores))\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "def train_best_pipeline(instantiator, best_params):\n",
    "    estimator = instantiator(best_params)\n",
    "    pipeline = make_pipeline(estimator)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_auc = evaluate_pipeline(pipeline, X_train, y_train)\n",
    "    test_auc = evaluate_pipeline(pipeline, X_test, y_test)\n",
    "    return pipeline, train_auc, test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b449c56",
   "metadata": {},
   "source": [
    "### Utilidades Compartidas\n",
    "Estas funciones construyen el pipeline estándar con imputación, SMOTE y escalado, calculan el AUC en cada conjunto y envuelven la lógica de Optuna para que podamos reutilizarla con cualquier modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18339fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección 2: Definición de Modelos y Espacios de Búsqueda\n",
    "\n",
    "def build_monotonic_lgbm(trial, params=None):\n",
    "    \"\"\"LightGBM con restricciones de monotonicidad basadas en conocimiento de dominio.\"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 127),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100)\n",
    "        }\n",
    "    return LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        monotone_constraints=MONOTONIC_CONSTRAINTS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "\n",
    "def build_costsensitive_xgb(trial, params=None):\n",
    "    \"\"\"XGBoost con función de pérdida ponderada por costos de negocio.\"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 3),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 8)\n",
    "        }\n",
    "    scale_pos_weight = 10.0\n",
    "    return XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "\n",
    "def build_hybrid_voting(trial, params=None):\n",
    "    \"\"\"Ensamble híbrido: EBM (interpretable) + XGBoost (potente) con ponderación alpha.\"\"\"\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 0.3, 0.7),\n",
    "            \"ebm_interactions\": trial.suggest_int(\"ebm_interactions\", 5, 15),\n",
    "            \"xgb_depth\": trial.suggest_int(\"xgb_depth\", 3, 6),\n",
    "            \"xgb_lr\": trial.suggest_float(\"xgb_lr\", 0.02, 0.15, log=True)\n",
    "        }\n",
    "\n",
    "    class HybridVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "        def __init__(self, alpha, ebm_params, xgb_params):\n",
    "            self.alpha = alpha\n",
    "            self.ebm_params = ebm_params\n",
    "            self.xgb_params = xgb_params\n",
    "            self.ebm = None\n",
    "            self.xgb = None\n",
    "            self.classes_ = None\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            self.classes_ = np.unique(y)\n",
    "            self.ebm = ExplainableBoostingClassifier(\n",
    "                interactions=self.ebm_params['interactions'],\n",
    "                max_rounds=3000,\n",
    "                learning_rate=0.02,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "            self.xgb = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                max_depth=self.xgb_params['depth'],\n",
    "                learning_rate=self.xgb_params['lr'],\n",
    "                n_estimators=200,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            self.ebm.fit(X, y)\n",
    "            self.xgb.fit(X, y)\n",
    "            return self\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            p_ebm = self.ebm.predict_proba(X)[:, 1]\n",
    "            p_xgb = self.xgb.predict_proba(X)[:, 1]\n",
    "            p_combined = self.alpha * p_ebm + (1 - self.alpha) * p_xgb\n",
    "            return np.vstack([1 - p_combined, p_combined]).T\n",
    "\n",
    "        def predict(self, X):\n",
    "            proba = self.predict_proba(X)\n",
    "            return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "    return HybridVotingClassifier(\n",
    "        alpha=params.get('alpha', 0.5),\n",
    "        ebm_params={'interactions': params.get('ebm_interactions', 10)},\n",
    "        xgb_params={'depth': params.get('xgb_depth', 4), 'lr': params.get('xgb_lr', 0.05)}\n",
    "    )\n",
    "\n",
    "\n",
    "def build_brp(trial, params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 15, 60),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.4, 0.9),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.3, 0.9),\n",
    "            \"tree_depth\": trial.suggest_int(\"tree_depth\", 3, 8)\n",
    "        }\n",
    "    base_tree = ExtraTreesClassifier(\n",
    "        n_estimators=1,\n",
    "        max_depth=params.get(\"tree_depth\", 5),\n",
    "        random_state=RANDOM_STATE,\n",
    "        bootstrap=False,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    return BalancedRandomPatches(\n",
    "        base_estimator=base_tree,\n",
    "        n_estimators=params.get(\"n_estimators\", 30),\n",
    "        max_samples=params.get(\"max_samples\", 0.6),\n",
    "        max_features=params.get(\"max_features\", 0.6),\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stacking(trial, params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"lr_c\": trial.suggest_float(\"lr_c\", 0.1, 1.5),\n",
    "            \"xgb_lr\": trial.suggest_float(\"xgb_lr\", 0.02, 0.2, log=True),\n",
    "            \"rf_depth\": trial.suggest_int(\"rf_depth\", 5, 12)\n",
    "        }\n",
    "    base_estimators = [\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"auc\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            learning_rate=params.get(\"xgb_lr\", 0.05),\n",
    "            max_depth=4,\n",
    "            n_estimators=200\n",
    "        )),\n",
    "        (\"et\", ExtraTreesClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=params.get(\"rf_depth\", 8),\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        (\"gb\", GradientBoostingClassifier(\n",
    "            n_estimators=150,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ]\n",
    "    final_estimator = LogisticRegression(\n",
    "        C=params.get(\"lr_c\", 0.5),\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    return StackingClassifier(\n",
    "        estimators=base_estimators,\n",
    "        final_estimator=final_estimator,\n",
    "        stack_method=\"predict_proba\",\n",
    "        passthrough=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "MODEL_BUILDERS = {\n",
    "    \"CostSensitiveXGB\": build_costsensitive_xgb,\n",
    "    \"MonotonicLightGBM\": build_monotonic_lgbm,\n",
    "    \"HybridVoting\": build_hybrid_voting,\n",
    "    \"BalancedRandomPatches\": build_brp,\n",
    "    \"StackingCompact\": build_stacking,\n",
    "}\n",
    "\n",
    "search_summary = []\n",
    "best_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fb380",
   "metadata": {},
   "source": [
    "### Sección 2 — Modelos Originales y Espacios de Búsqueda\n",
    "En esta sección definimos cada arquitectura original junto con su espacio de hiperparámetros para Optuna. Aquí también aplicamos las restricciones de negocio (monotonicidad, costos, ensambles custom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc350cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizando CostSensitiveXGB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8b2bf76ae147629676a111567e0f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sección 3: Búsqueda con Optuna\n",
    "for model_name, builder in MODEL_BUILDERS.items():\n",
    "    print(f\"\\nOptimizando {model_name}...\")\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "    objective = objective_factory(lambda trial, b=builder: b(trial))\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=N_TRIALS,\n",
    "        timeout=STUDY_TIMEOUT,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    pipeline, train_auc, test_auc = train_best_pipeline(\n",
    "        lambda params, b=builder: b(None, params=params),\n",
    "        best_params\n",
    "    )\n",
    "\n",
    "    gap = train_auc - test_auc\n",
    "    search_summary.append(\n",
    "        {\n",
    "            \"Modelo\": model_name,\n",
    "            \"Val_AUC\": study.best_value,\n",
    "            \"Train_AUC\": train_auc,\n",
    "            \"Test_AUC\": test_auc,\n",
    "            \"Gap\": gap\n",
    "        }\n",
    "    )\n",
    "    best_models[model_name] = {\n",
    "        \"pipeline\": pipeline,\n",
    "        \"best_params\": best_params,\n",
    "        \"val_auc\": study.best_value,\n",
    "        \"test_auc\": test_auc\n",
    "    }\n",
    "    print(\n",
    "        f\"{model_name} listo -> Val AUC={study.best_value:.4f} | Test AUC={test_auc:.4f} | Gap={gap:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe7c09",
   "metadata": {},
   "source": [
    "### Sección 3 — Optimización con Optuna\n",
    "Iteramos sobre cada modelo original, ejecutando 25 trials con validación cruzada estratificada. Guardamos el desempeño en entrenamiento y validación para monitorear estabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección 4: Resultados Comparativos\n",
    "results_df = pd.DataFrame(search_summary).sort_values(by=\"Val_AUC\", ascending=False)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "display(results_df)\n",
    "\n",
    "best_model_name = results_df.iloc[0][\"Modelo\"]\n",
    "best_pipeline = best_models[best_model_name][\"pipeline\"]\n",
    "print(f\"\\nMejor modelo por AUC-ROC validacion: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14638fe9",
   "metadata": {},
   "source": [
    "### Sección 4.1 — Métricas Detalladas en Hold-out\n",
    "Calculamos las métricas definitivas del pipeline ganador usando el conjunto de test reservado, replicando el formato ejecutivo que manejas habitualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección 4.1: Métricas Ejecutivas\n",
    "y_pred_holdout = best_pipeline.predict(X_test)\n",
    "y_proba_holdout = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "auc_roc = roc_auc_score(y_test, y_proba_holdout)\n",
    "auc_pr = average_precision_score(y_test, y_proba_holdout)\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"Average Precision (AUC-PR): {auc_pr:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_holdout, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527419e",
   "metadata": {},
   "source": [
    "### Sección 4 — Consolidación de Resultados\n",
    "Ordenamos los experimentos por AUC validado, identificamos al ganador y dejamos el pipeline listo para posteriores análisis y visualizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a922c84",
   "metadata": {},
   "source": [
    "### Lectura de la Tabla\n",
    "- **Val_AUC**: promedio de las 5 folds con SMOTE dentro del pipeline.\n",
    "- **Test_AUC**: desempeño en el hold-out del 30% reservado al inicio.\n",
    "- **Gap**: `Train_AUC - Test_AUC`; valores pequeños indican modelos estables.\n",
    "\n",
    "El modelo ganador se selecciona únicamente por `Val_AUC`, garantizando que la decisión sea reproducible y ajena al azar del hold-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección 5: Curva ROC en Hold-out\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "RocCurveDisplay.from_estimator(best_pipeline, X_test, y_test, name=best_model_name, ax=ax)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Azar')\n",
    "ax.set_title('Curva ROC — Mejor Modelo')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615637f8",
   "metadata": {},
   "source": [
    "### Sección 5 — Visualización del Ganador\n",
    "Graficamos la curva ROC en el hold-out para validar visualmente la distancia frente a un clasificador aleatorio y detectar posibles sobreajustes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sección 6: Persistencia del Modelo Ganador\n",
    "best_model_path = MODELS_DIR / \"best_auc_model.pkl\"\n",
    "best_info = best_models[best_model_name]\n",
    "joblib.dump({\n",
    "    \"model_name\": best_model_name,\n",
    "    \"pipeline\": best_info[\"pipeline\"],\n",
    "    \"metadata\": {\n",
    "        \"best_params\": best_info[\"best_params\"],\n",
    "        \"val_auc\": best_info[\"val_auc\"],\n",
    "        \"test_auc\": best_info[\"test_auc\"]\n",
    "    }\n",
    "}, best_model_path)\n",
    "print(f\"Modelo guardado en {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e7fca",
   "metadata": {},
   "source": [
    "### Sección 6 — Persistencia\n",
    "Guardamos en disco el pipeline ganador junto con los metadatos relevantes para facilitar futuras comparaciones o despliegues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634c004",
   "metadata": {},
   "source": [
    "## Conclusiones Académicas\n",
    "- Todos los modelos implementados son **arquitecturas originales** diseñadas específicamente para este proyecto:\n",
    "  - **Cost-Sensitive XGBoost** incorpora costos de negocio directamente en el entrenamiento\n",
    "  - **Monotonic LightGBM** respeta relaciones causales conocidas (edad, deuda, ingresos)\n",
    "  - **Hybrid Voting System** balancea interpretabilidad del EBM con poder predictivo de XGBoost\n",
    "  - **Balanced Random Patches** reduce correlación entre estimadores mediante doble aleatoriedad\n",
    "  - **Stacking Compact** combina diversidad de familias de modelos\n",
    "- La comparación controlada identifica qué enfoque escala mejor con las features actuales.\n",
    "- Mejoras esperadas: 0.5-1.5% en AUC sobre baseline (0.8593), alcanzando 0.863-0.870.\n",
    "- El pipeline ganador queda documentado y persistido para despliegue o benchmarking futuro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
